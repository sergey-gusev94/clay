#!/usr/bin/env python3
"""
Analysis script for constrained layout (clay) optimization results.

This script reads the clay_results.xlsx file generated by clay.py and creates:
1. Performance profiles for each solver-subsolver combination
2. Outcome bar plots (optimal, timeout, wrong results)
3. Text files with outcome summaries

Author: Generated for constrained layout problem analysis
"""

import os
from datetime import datetime
from typing import Dict, List, Optional

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd


def _get_strategy_display_name(strategy: str) -> str:
    """Gets a display-friendly name for a strategy."""
    display_name = strategy.replace("gdp.", "")
    # Longer names must be replaced first to avoid partial replacements
    display_name = display_name.replace("hull_exact", "Hull Exact")
    display_name = display_name.replace("hull_reduced_y", "Hull Reduced Y")
    display_name = display_name.replace("binary_multiplication", "Binary Mult.")
    display_name = display_name.replace("hull", "Hull(ε-approx.)")
    display_name = display_name.replace("bigm", "BigM")
    return display_name


def _filter_strategies(
    strategies: list, 
    include_strategies: Optional[list] = None,
    exclude_strategies: Optional[list] = None
) -> list:
    """Filter strategies based on include/exclude lists."""
    if include_strategies is not None:
        strategies = [s for s in strategies if s in include_strategies]
    if exclude_strategies is not None:
        strategies = [s for s in strategies if s not in exclude_strategies]
    return strategies


def _get_strategy_style_maps() -> tuple:
    """Get consistent style and color mappings for strategies."""
    style_map = {
        "gdp.bigm": "-",
        "gdp.hull": "--",
        "gdp.hull_exact": "-.",
        "gdp.hull_reduced_y": ":",
        "gdp.binary_multiplication": (0, (5, 1)),
    }
    color_map = {
        "gdp.bigm": "blue",
        "gdp.hull": "brown",
        "gdp.hull_exact": "green",
        "gdp.hull_reduced_y": "purple",
        "gdp.binary_multiplication": "orange",
    }
    return style_map, color_map


def _is_solution_correct(
    row: pd.Series, 
    ground_truth: dict, 
    obj_tolerance: float,
    time_limit: float
) -> bool:
    """Check if a solution is correct (optimal, within time limit, and correct objective)."""
    duration = row["Duration (sec)"]
    status = row.get("Status", "unknown")
    problem_name = row["Problem Name"]
    metric = row["Metric"]
    
    # Must be within time limit and optimal status
    if duration >= time_limit or status != "optimal":
        return False
    
    # Check objective value if available
    key = (problem_name, metric)
    if key in ground_truth and "Objective Value" in row:
        ground_truth_val = ground_truth[key]
        computed_val = float(row["Objective Value"])
        
        # Calculate relative error for clay problems
        if abs(ground_truth_val) > 1e-12:
            rel_error = abs(computed_val - ground_truth_val) / abs(ground_truth_val)
            return bool(rel_error <= obj_tolerance)
        else:
            # For near-zero ground truth, use absolute tolerance
            return bool(abs(computed_val - ground_truth_val) <= obj_tolerance)
    
    return True


def create_dolan_more_performance_profile(
    df: pd.DataFrame,
    output_dir: str,
    solver_combo: str,
    time_limit: float = 1800,
    obj_tolerance: float = 1e-4,
    exclude_strategies: Optional[List[str]] = None,
    include_strategies: Optional[List[str]] = None,
    output_suffix: str = "",
) -> None:
    """
    Create standard Dolan-Moré performance profiles.
    
    For each problem instance, calculates the performance ratio rm = t_m,s / min_s(t_m,s)
    where t_m,s is the time for method s on problem m.
    Failed cases (timeout, wrong solution, missing data) are assigned rm = time_limit / min_time.
    
    Parameters
    ----------
    df : pd.DataFrame
        DataFrame containing the results
    output_dir : str
        Directory to save the plots
    solver_combo : str
        Solver combination name for titles
    time_limit : float, optional
        Time limit in seconds, by default 1800 (30 minutes)
    obj_tolerance : float, optional
        Relative error tolerance for determining if objective values are different, by default 1e-4
    exclude_strategies : List[str], optional
        List of strategies to exclude, by default None
    include_strategies : List[str], optional
        List of strategies to include (if None, include all strategies), by default None
    output_suffix : str, optional
        Suffix to add to output filenames, by default ""
    """
    print(f"Creating Dolan-Moré performance profiles for {solver_combo}...")
    
    # Apply strategy filtering
    strategies = _filter_strategies(
        df["Strategy"].unique().tolist(), include_strategies, exclude_strategies
    )
        
    if not strategies:
        print("No strategies available after filtering, skipping Dolan-Moré profiles")
        return
    
    # Get all unique problem-metric combinations
    problem_metrics = df[["Problem Name", "Metric"]].drop_duplicates()
    
    # Calculate ground truth for objective value filtering
    ground_truth = {}
    if "Objective Value" in df.columns:
        optimal_solutions = df[df["Status"] == "optimal"]
        ground_truth_series = optimal_solutions.groupby(["Problem Name", "Metric"])["Objective Value"].min()
        ground_truth = ground_truth_series.to_dict()
    
    # For each problem-metric combination, collect valid solution times for each strategy
    problem_times: Dict[tuple, Dict[str, Optional[float]]] = {}
    min_valid_time = float("inf")
    
    for _, row in problem_metrics.iterrows():
        problem_name = row["Problem Name"]
        metric = row["Metric"]
        key = (problem_name, metric)
        
        problem_data = df[(df["Problem Name"] == problem_name) & (df["Metric"] == metric)]
        problem_times[key] = {}
        
        for strategy in strategies:
            strategy_data = problem_data[problem_data["Strategy"] == strategy]
            
            if len(strategy_data) == 0:
                # Missing data - will be assigned failure ratio
                problem_times[key][strategy] = None
                continue
                
            row_data = strategy_data.iloc[0]  # Assume one entry per problem-strategy pair
            
            # Check if solution is correct using helper function
            if _is_solution_correct(row_data, ground_truth, obj_tolerance, time_limit):
                duration = row_data["Duration (sec)"]
                problem_times[key][strategy] = duration
                min_valid_time = min(min_valid_time, duration)
            else:
                # Timeout, wrong solution, or other failure
                problem_times[key][strategy] = None
    
    if min_valid_time == float("inf"):
        print("No valid solutions found, cannot create Dolan-Moré profiles")
        return
    
    # Calculate failure ratio
    failure_ratio = time_limit / min_valid_time
    
    # Calculate performance ratios for each strategy
    strategy_ratios: Dict[str, List[float]] = {strategy: [] for strategy in strategies}
    
    for key in problem_times:
        # Find minimum time for this problem across all strategies
        valid_times = [t for t in problem_times[key].values() if t is not None]
        if not valid_times:
            # No strategy solved this problem - skip it
            continue
            
        min_time_for_problem = min(valid_times)
        
        for strategy in strategies:
            time_s = problem_times[key][strategy]
            if time_s is not None:
                ratio = time_s / min_time_for_problem
            else:
                ratio = failure_ratio
            strategy_ratios[strategy].append(ratio)
    
    # Create the plot
    plt.figure(figsize=(12, 8))
    
    # Get consistent style and color mappings
    style_map, color_map = _get_strategy_style_maps()
    
    for strategy in strategies:
        ratios = sorted(strategy_ratios[strategy])
        n_problems = len(ratios)
        
        # Create x values (performance ratios) and y values (fraction of problems solved)
        y_values = np.arange(1, n_problems + 1) / n_problems
        
        style = style_map.get(strategy, "-")
        color = color_map.get(strategy, "black")
        display_name = _get_strategy_display_name(strategy)
        
        plt.step(ratios, y_values, where="post", linewidth=6, 
                linestyle=style, color=color, label=display_name)
    
    plt.xlabel("Performance Ratio", fontsize=34)
    plt.ylabel("Fraction of Instances Solved", fontsize=32)
    plt.xscale("log")
    plt.xlim(1, failure_ratio * 1.1)
    plt.ylim(0, 1)
    plt.grid(True, alpha=0.3)
    plt.legend(loc="lower right", fontsize=20, framealpha=0.4)
    plt.tick_params(axis="both", which="major", labelsize=24)
    plt.tight_layout()
    
    # Save the figure
    dolan_suffix = f"_{output_suffix}" if output_suffix else ""
    output_file = os.path.join(output_dir, f"dolan_more_profile_{solver_combo}{dolan_suffix}.jpg")
    plt.savefig(output_file, dpi=300, bbox_inches="tight")
    plt.close()
    print(f"Saved Dolan-Moré profile to {output_file}")


def create_performance_profile(
    df: pd.DataFrame,
    output_dir: str,
    solver_combo: str,
    time_limit: float = 1800,
    obj_tolerance: float = 1e-4,
    exclude_strategies: Optional[List[str]] = None,
    include_strategies: Optional[List[str]] = None,
    output_suffix: str = "",
) -> None:
    """
    Create performance profiles showing number of instances solved within a time limit.

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame containing the results
    output_dir : str
        Directory to save the plots
    solver_combo : str
        Solver combination name for titles
    time_limit : float, optional
        Time limit in seconds, by default 1800 (30 minutes)
    obj_tolerance : float, optional
        Relative error tolerance for determining if objective values are different, by default 1e-4
    exclude_strategies : List[str], optional
        List of strategies to exclude from plots, by default None
    include_strategies : List[str], optional
        List of strategies to include (if None, include all strategies), by default None
    output_suffix : str, optional
        Suffix to add to output filenames, by default ""
    """
    print(f"Creating performance profiles for {solver_combo}...")

    # Filter out instances with objective values differing from ground truth beyond tolerance
    if "Objective Value" in df.columns:
        # Determine ground truth as the minimum objective value for each problem-metric combination
        ground_truth = df.groupby(["Problem Name", "Metric"])["Objective Value"].min()
        
        # Only apply filtering to data where Duration (sec) < time_limit
        mask_below_limit = df["Duration (sec)"] < time_limit
        df_below = df[mask_below_limit].copy()
        df_above = df[~mask_below_limit].copy()
        
        if not df_below.empty:
            df_below = df_below.assign(
                GroundTruth=df_below.apply(
                    lambda row: ground_truth.get((row["Problem Name"], row["Metric"]), np.nan), 
                    axis=1
                )
            )
            # Calculate relative error: |computed - ground_truth| / |ground_truth|
            ground_truth_values = df_below["GroundTruth"]
            computed_values = df_below["Objective Value"]
            
            # Handle cases where ground truth is zero to avoid division by zero
            non_zero_gt = np.abs(ground_truth_values) > 1e-12
            rel_error = np.full(len(df_below), np.inf)  # Initialize with inf for zero ground truth
            rel_error[non_zero_gt] = (
                np.abs(computed_values[non_zero_gt] - ground_truth_values[non_zero_gt]) 
                / np.abs(ground_truth_values[non_zero_gt])
            )
            
            valid_mask = rel_error <= obj_tolerance
            num_filtered = (~valid_mask).sum()
            
            if not valid_mask.any():
                print(
                    "No data within objective tolerance for solutions below time limit,\
                          skipping performance profiles"
                )
                return
            if num_filtered > 0:
                print(
                    f"Filtered out {num_filtered} entries with objective values outside \
                        relative error tolerance {obj_tolerance} (for solutions below time limit)"
                )
            df_below = df_below[valid_mask]
        
        # Concatenate filtered below-limit and unfiltered above-limit data
        df = pd.concat([df_below, df_above], ignore_index=True)
    else:
        print("Objective Value column not found, unable to filter based on objective tolerance")

    # Get unique strategies
    strategies = df["Strategy"].unique()
    print(f"All unique strategies found in data: {list(strategies)}")
    
    # Apply strategy filtering
    original_strategies = list(strategies)
    strategies = _filter_strategies(strategies, include_strategies, exclude_strategies)
    
    if include_strategies is not None:
        print(f"Filtering to include only strategies: {include_strategies}")
        print(f"Original strategies: {original_strategies}")
        print(f"Available strategies after filtering: {list(strategies)}")

    # If no strategies match the filter, warn and return early
    if not strategies:
        print(
            "Warning: No strategies match the filtering criteria. \
                Skipping profile generation."
        )
        return

    # Get consistent style and color mappings
    style_map, color_map = _get_strategy_style_maps()

    # Create individual performance profiles
    for strategy in strategies:
        print(f"  Creating profile for {strategy}")
        strategy_data = df[df["Strategy"] == strategy]

        # Sort by solution time
        solution_times = sorted(strategy_data["Duration (sec)"])

        # Create x and y arrays for plotting
        x = np.sort(solution_times)
        y = np.arange(1, len(x) + 1)

        # Create the plot
        plt.figure(figsize=(10, 6))
        plt.step(x, y, where="post", linewidth=4)

        # Add time limit line
        plt.axvline(
            x=time_limit, color="r", linestyle="--", alpha=0.7, label=f"Time limit {time_limit}s"
        )

        plt.xlabel("Solution Time (s)", fontsize=32)
        plt.ylabel("Number of Instances Solved", fontsize=32)
        # plt.title(f"Performance Profile: {_get_strategy_display_name(strategy)} ({solver_combo})", fontsize=34)

        # Add grid
        plt.grid(True, alpha=0.3)

        # Set x-axis to log scale
        plt.xscale("log")

        # Add legend in bottom right corner
        plt.legend(loc="lower right", fontsize=24, framealpha=0.4)

        # Adjust layout to prevent label cutoff
        plt.tight_layout()

        # Save the figure
        strategy_suffix = f"_{output_suffix}" if output_suffix else ""
        output_file = os.path.join(output_dir, f"profile_{strategy}_{solver_combo}{strategy_suffix}.jpg")
        plt.savefig(output_file, dpi=300, bbox_inches="tight")
        plt.close()
        print(f"  Saved to {output_file}")

    # Create combined performance profile
    plt.figure(figsize=(12, 8))

    # Plot each strategy with its assigned style and color
    strategy_lines = []
    strategy_labels = []
    for strategy in strategies:
        strategy_data = df[df["Strategy"] == strategy]
        solution_times = sorted(strategy_data["Duration (sec)"])
        x = np.sort(solution_times)
        y = np.arange(1, len(x) + 1)

        style = style_map.get(strategy, "-")
        color = color_map.get(strategy, "black")

        display_name = _get_strategy_display_name(strategy)

        (line,) = plt.step(
            x, y, where="post", linewidth=8, linestyle=style, color=color, label=display_name
        )
        strategy_lines.append(line)
        strategy_labels.append(display_name)

    # Add time limit line
    time_limit_line = plt.axvline(
        x=time_limit,
        color="r",
        linestyle="--",
        alpha=0.7,
        linewidth=6,
        label=f"Time limit {time_limit}s",
    )

    plt.xlabel("Solution Time (s)", fontsize=34)
    plt.ylabel("Number of Instances Solved", fontsize=34)
    # plt.title(f"Performance Profiles ({solver_combo})", fontsize=38)

    # Single legend for everything (upper left)
    plt.legend(loc="upper left", fontsize=19, framealpha=0.4)

    plt.grid(True, alpha=0.3)
    plt.tick_params(axis="both", which="major", labelsize=24)

    # Set x-axis to log scale
    plt.xscale("log")

    # Adjust layout to prevent label cutoff
    plt.tight_layout()

    # Save the figure
    combined_suffix = f"_{output_suffix}" if output_suffix else ""
    output_file = os.path.join(output_dir, f"profile_combined_{solver_combo}{combined_suffix}.jpg")
    plt.savefig(output_file, dpi=300, bbox_inches="tight")
    plt.close()
    print(f"Saved combined profile to {output_file}")


def create_outcome_analysis(
    df: pd.DataFrame,
    output_dir: str,
    solver_combo: str,
    time_limit: float = 1800,
    obj_tolerance: float = 1e-4,
    exclude_strategies: Optional[List[str]] = None,
) -> None:
    """
    Create outcome analysis including bar plots and text summaries.

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame containing the results
    output_dir : str
        Directory to save the outputs
    solver_combo : str
        Solver combination name
    time_limit : float, optional
        Time limit in seconds, by default 1800
    obj_tolerance : float, optional
        Relative error tolerance for determining correct solutions, by default 1e-4
    exclude_strategies : List[str], optional
        List of strategies to exclude, by default None
    """
    print(f"Creating outcome analysis for {solver_combo}...")

    # Get unique strategies
    strategies = df["Strategy"].unique()
    
    # Filter strategies if exclude_strategies is provided
    if exclude_strategies is not None:
        strategies = [s for s in strategies if s not in exclude_strategies]

    # Compute ground truth for each problem-metric combination
    if "Objective Value" in df.columns:
        # Only consider optimal solutions for ground truth
        optimal_solutions = df[df["Status"] == "optimal"]
        ground_truth = optimal_solutions.groupby(["Problem Name", "Metric"])["Objective Value"].min()
    else:
        ground_truth = pd.Series(dtype=float)

    # Initialize counts and create a list to store results for text file
    counts_opt = []
    counts_timeout = []
    counts_wrong = []
    results_data = []

    for strategy in strategies:
        s_df = df[df["Strategy"] == strategy]
        
        # Number of timeouts
        n_timeout = (s_df["Duration (sec)"] >= time_limit).sum()
        
        # Entries that didn't timeout
        s_not_timeout = s_df[s_df["Duration (sec)"] < time_limit]
        
        # Determine correct optimal solutions
        if "Status" in s_not_timeout.columns and "Objective Value" in s_not_timeout.columns:
            gt = s_not_timeout.apply(
                lambda row: ground_truth.get((row["Problem Name"], row["Metric"]), np.nan),
                axis=1
            )
            # Calculate relative error for determining correct solutions
            computed_values = s_not_timeout["Objective Value"]
            ground_truth_values = gt
            
            # Handle cases where ground truth is zero to avoid division by zero
            non_zero_gt = np.abs(ground_truth_values) > 1e-12
            rel_error = np.full(len(s_not_timeout), np.inf)  # Initialize with inf for zero ground truth
            rel_error[non_zero_gt] = (
                np.abs(computed_values[non_zero_gt] - ground_truth_values[non_zero_gt]) 
                / np.abs(ground_truth_values[non_zero_gt])
            )
            
            correct_opt = s_not_timeout[
                (s_not_timeout["Status"] == "optimal")
                & (rel_error <= obj_tolerance)
            ]
            n_opt = len(correct_opt)

            # Print details about wrong solutions
            wrong_solutions = s_not_timeout[
                ~(
                    (s_not_timeout["Status"] == "optimal")
                    & (rel_error <= obj_tolerance)
                )
            ]
            if len(wrong_solutions) > 0:
                print(f"\nWrong solutions for {strategy}:")
                for idx, row in wrong_solutions.iterrows():
                    problem_name = row["Problem Name"]
                    metric = row["Metric"]
                    reformulation_obj = row["Objective Value"]
                    ground_truth_obj = ground_truth.get((problem_name, metric), np.nan)
                    print(f"  Problem: {problem_name} (metric: {metric})")
                    print(f"    Reformulation objective: {reformulation_obj}")
                    print(f"    Ground truth objective: {ground_truth_obj}")
                    if not np.isnan(ground_truth_obj) and abs(ground_truth_obj) > 1e-12:
                        abs_diff = abs(reformulation_obj - ground_truth_obj)
                        rel_err = abs_diff / abs(ground_truth_obj)
                        print(f"    Absolute difference: {abs_diff}")
                        print(f"    Relative error: {rel_err:.6e} ({rel_err*100:.4f}%)")
                    elif not np.isnan(ground_truth_obj):
                        print(f"    Absolute difference: {abs(reformulation_obj - ground_truth_obj)} (ground truth near zero)")
                    print(f"    Status: {row['Status']}")
                    print()
        else:
            n_opt = 0
        
        # Wrong solutions: non-timeouts that are not correct optimal
        n_wrong = len(s_not_timeout) - n_opt
        counts_opt.append(n_opt)
        counts_timeout.append(n_timeout)
        counts_wrong.append(n_wrong)

        # Store results for text file
        display_name = _get_strategy_display_name(strategy)

        results_data.append(
            {
                "Strategy": display_name,
                "Optimal": n_opt,
                "Timeout": n_timeout,
                "Wrong": n_wrong,
                "Total": n_opt + n_timeout + n_wrong,
            }
        )

    # Save results to text file
    txt_output = os.path.join(output_dir, f"solution_outcomes_{solver_combo}.txt")
    with open(txt_output, "w") as f:
        f.write(f"Solution Outcomes Summary - {solver_combo}\n")
        f.write("=" * (30 + len(solver_combo)) + "\n\n")
        f.write(f"Time limit: {time_limit} seconds\n")
        f.write(f"Relative error tolerance: {obj_tolerance}\n\n")

        # Write table header
        f.write(f"{'Strategy':<20} {'Optimal':<10} {'Timeout':<10} {'Wrong':<10} {'Total':<10}\n")
        f.write("-" * 60 + "\n")

        # Write data rows
        for data in results_data:
            f.write(
                f"{data['Strategy']:<20} {data['Optimal']:<10} {data['Timeout']:<10} "
                f"{data['Wrong']:<10} {data['Total']:<10}\n"
            )

    print(f"Saved solution outcomes summary to {txt_output}")

    # Plot bar chart
    x = np.arange(len(strategies))
    width = 0.2
    plt.figure(figsize=(10, 6))
    plt.bar(x - width, counts_opt, width, label="Optimal", color="green")
    plt.bar(x, counts_timeout, width, label="Timeout", color="orange")
    plt.bar(x + width, counts_wrong, width, label="Wrong Solution", color="red")
    plt.xlabel("Strategy", fontsize=30)
    plt.ylabel("Count", fontsize=30)
    # plt.title(f"Solution Outcomes by Strategy ({solver_combo})", fontsize=32)
    display_names = [_get_strategy_display_name(s) for s in strategies]
    plt.xticks(x, display_names, rotation=0, fontsize=22)
    plt.yticks(fontsize=18)
    plt.legend(loc="upper center", bbox_to_anchor=(0.32, 1), fontsize=22, framealpha=0.5)
    plt.grid(axis="y", alpha=0.3)
    bar_output = os.path.join(output_dir, f"solution_outcomes_bar_{solver_combo}.jpg")
    plt.tight_layout()
    plt.savefig(bar_output, dpi=300, bbox_inches="tight")
    plt.close()
    print(f"Saved bar plot to {bar_output}")


def create_solution_time_comparison(
    df: pd.DataFrame,
    strategy1: str,
    strategy2: str,
    output_dir: str,
    solver_combo: str,
    obj_tolerance: float = 1e-4,
    time_limit: float = 1800,
) -> None:
    """
    Create a scatter plot comparing solution times between two strategies.

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame containing the results
    strategy1 : str
        First strategy to compare
    strategy2 : str
        Second strategy to compare
    output_dir : str
        Directory to save the plot
    solver_combo : str
        Solver combination name
    obj_tolerance : float, optional
        Relative error tolerance for determining if objective values are different, by default 1e-4
    time_limit : float, optional
        Time limit in seconds, by default 1800
    """
    print(f"Creating comparison plot: {strategy1} vs {strategy2} for {solver_combo}")

    # Get display names for strategies
    display_name1 = _get_strategy_display_name(strategy1)
    display_name2 = _get_strategy_display_name(strategy2)

    # Filter data for the two strategies
    df1 = df[df["Strategy"] == strategy1]
    df2 = df[df["Strategy"] == strategy2]

    # Merge on Problem Name and Metric to get matching pairs
    merged = pd.merge(df1, df2, on=["Problem Name", "Metric"], suffixes=("_1", "_2"))

    if len(merged) == 0:
        print(f"No matching instances found for {strategy1} vs {strategy2}")
        return

    # Extract solution times
    times1 = merged["Duration (sec)_1"]
    times2 = merged["Duration (sec)_2"]

    # Extract objective values
    if "Objective Value_1" in merged.columns and "Objective Value_2" in merged.columns:
        obj1 = merged["Objective Value_1"]
        obj2 = merged["Objective Value_2"]

        # Calculate relative error between the two strategies
        # Use the smaller absolute value as the reference to avoid bias
        ref_values = np.minimum(np.abs(obj1), np.abs(obj2))
        non_zero_ref = ref_values > 1e-12
        
        rel_error = np.full(len(merged), np.inf)  # Initialize with inf for zero reference
        rel_error[non_zero_ref] = (
            np.abs(obj1[non_zero_ref] - obj2[non_zero_ref]) 
            / ref_values[non_zero_ref]
        )
        
        # Check if objective values are different within relative tolerance
        obj_different = rel_error > obj_tolerance
    else:
        # If objective values don't exist, assume all are the same
        obj_different = np.zeros(len(merged), dtype=bool)

    # Create the plot
    plt.figure(figsize=(12, 8))

    # Determine max value for axis limits
    max_time = max(times1.max(), times2.max(), time_limit) * 1.2  # Add 20% margin

    # Plot diagonal line (x=y)
    plt.plot([0, time_limit], [0, time_limit], "k--", alpha=0.5, linewidth=6, label="x=y")

    # Add time limit line
    plt.axhline(
        y=time_limit,
        color="r",
        linestyle="--",
        alpha=0.7,
        linewidth=6,
        label=f"Time limit {time_limit}s",
    )
    plt.axvline(x=time_limit, color="r", linestyle="--", alpha=0.7, linewidth=6)

    # Plot data points with different colors based on objective value difference
    blue_points = ~obj_different
    red_points = obj_different

    if np.any(blue_points):
        plt.scatter(
            times1[blue_points],
            times2[blue_points],
            alpha=0.7,
            s=100,
            color="blue",
            label="Same objective",
        )

    if np.any(red_points):
        plt.scatter(
            times1[red_points],
            times2[red_points],
            alpha=0.7,
            s=100,
            color="red",
            label="Different objective",
        )

    # Add labels and title
    plt.xlabel(f"{display_name1} Solution Time (s)", fontsize=28)
    plt.ylabel(f"{display_name2} Solution Time (s)", fontsize=28)
    # plt.title(f"Solution Time Comparison\n{display_name1} vs {display_name2} ({solver_combo})", fontsize=32)

    # Add legend
    plt.legend(loc="upper left", fontsize=26, framealpha=0.4)

    # Add grid
    plt.grid(True, alpha=0.3)

    # Ensure axes are equal and have the same limits
    plt.xlim(0, max_time)
    plt.ylim(0, max_time)

    # Set log scale for both axes
    plt.xscale("log")
    plt.yscale("log")

    # Ensure axes are symmetric by setting identical limits
    min_time = max(0.1, min(times1.min(), times2.min()) * 0.9)  # Avoid log(0) issues
    plt.xlim(min_time, max_time)
    plt.ylim(min_time, max_time)

    # Set tick parameters
    plt.tick_params(axis="both", which="major", labelsize=24)

    # Adjust layout to prevent label cutoff
    plt.tight_layout()

    # Save the figure
    output_file = os.path.join(output_dir, f"comparison_{strategy1}_vs_{strategy2}_{solver_combo}.jpg")
    plt.savefig(output_file, dpi=300, bbox_inches="tight")
    plt.close()
    print(f"Saved plot to {output_file}")


def main() -> None:
    """Main function to analyze clay results and generate plots."""
    # Define paths
    script_dir = os.path.dirname(os.path.abspath(__file__))
    data_dir = os.path.join(script_dir, "data")
    results_file = os.path.join(data_dir, "clay_results.xlsx")
    plots_base_dir = os.path.join(data_dir, "plots")

    # Define time limit in seconds (30 minutes)
    time_limit = 1800

    # Create plots base directory if it doesn't exist
    os.makedirs(plots_base_dir, exist_ok=True)

    # Create a numbered subfolder to avoid overwriting existing plots
    counter = 1
    while True:
        plots_dir = os.path.join(plots_base_dir, str(counter))
        if not os.path.exists(plots_dir):
            os.makedirs(plots_dir)
            print(f"Created plots directory: {plots_dir}")
            break
        counter += 1

    # Create a README file with timestamp and info
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    with open(os.path.join(plots_dir, "README.txt"), "w") as readme:
        readme.write(f"Clay Results Analysis - Generated on: {current_time}\n")
        readme.write(f"Directory: {plots_dir}\n\n")
        readme.write("Contains:\n")
        readme.write("1. Performance profiles for each solver-subsolver combination\n")
        readme.write("2. Dolan-Moré performance profiles (standard performance ratios)\n")
        readme.write("3. Solution outcome bar plots\n")
        readme.write("4. Text summaries of outcomes\n")
        readme.write("5. Solution time comparison plots between strategies\n\n")
        readme.write("Generated by analyze_clay_results.py\n")

    # Check if results file exists
    if not os.path.exists(results_file):
        print(f"Error: Results file not found at {results_file}")
        return

    # Read the Excel file
    print(f"Reading results from {results_file}")
    df = pd.read_excel(results_file)

    print("\nData summary:")
    print(f"Total entries: {len(df)}")
    print(f"Unique problems: {df['Problem Name'].nunique()}")
    print(f"Strategies: {df['Strategy'].unique()}")
    print(f"Solvers: {df['Solver'].unique()}")
    print(f"Subsolvers: {df['Subsolver'].unique()}")
    print(f"Metrics: {df['Metric'].unique()}")

    # Create solver-subsolver combinations
    df["Solver_Combo"] = df["Solver"] + "_" + df["Subsolver"].fillna("direct")
    solver_combos = df["Solver_Combo"].unique()

    print(f"Solver combinations: {solver_combos}")

    # For each solver-subsolver combination, create a separate set of plots
    for solver_combo in solver_combos:
        solver_dir = os.path.join(plots_dir, solver_combo)
        os.makedirs(solver_dir, exist_ok=True)

        print(f"\nGenerating analysis for solver combination: {solver_combo}")

        # Filter data for this solver-subsolver combination
        solver_df = df[df["Solver_Combo"] == solver_combo]

        # Create performance profiles
        create_performance_profile(
            solver_df,
            solver_dir,
            solver_combo,
            time_limit=time_limit,
            exclude_strategies=["gdp.hull_reduced_y"]  # Exclude if it causes issues
        )

        # Generate performance profiles for specified reformulations only
        print("\nGenerating performance profiles for specific reformulations...")
        print(f"Available strategies in solver_df: {list(solver_df['Strategy'].unique())}")
        specified_reformulations = ["gdp.hull_exact", "gdp.hull"]
        print(f"Requested strategies: {specified_reformulations}")
        create_performance_profile(
            solver_df,
            solver_dir,
            solver_combo,
            time_limit=time_limit,
            include_strategies=specified_reformulations,
            output_suffix="hull_exact_vs_hull",
        )

        # Generate Dolan-Moré performance profiles
        print("\nGenerating Dolan-Moré performance profiles...")
        create_dolan_more_performance_profile(
            solver_df,
            solver_dir,
            solver_combo,
            time_limit=time_limit,
            exclude_strategies=["gdp.hull_reduced_y"]
        )

        # Generate Dolan-Moré performance profiles for specified reformulations only
        print("\nGenerating Dolan-Moré performance profiles for specific reformulations...")
        create_dolan_more_performance_profile(
            solver_df,
            solver_dir,
            solver_combo,
            time_limit=time_limit,
            include_strategies=specified_reformulations,
            output_suffix="hull_exact_vs_hull",
        )

        # Create outcome analysis
        create_outcome_analysis(
            solver_df,
            solver_dir,
            solver_combo,
            time_limit=time_limit,
            exclude_strategies=["gdp.hull_reduced_y"]  # Exclude if it causes issues
        )

        # Define strategy comparisons to plot
        strategies = solver_df["Strategy"].unique()
        comparisons = [
            ("gdp.bigm", "gdp.hull"),
            ("gdp.hull_exact", "gdp.hull"),
            ("gdp.bigm", "gdp.hull_exact"),
            ("gdp.hull_exact", "gdp.binary_multiplication"),
            ("gdp.bigm", "gdp.binary_multiplication"),
        ]

        # Generate comparison plots
        print("\nGenerating comparison plots...")
        for strategy1, strategy2 in comparisons:
            if strategy1 in strategies and strategy2 in strategies:
                create_solution_time_comparison(
                    solver_df,
                    strategy1,
                    strategy2,
                    solver_dir,
                    solver_combo,
                    obj_tolerance=1e-4,  # Relative error tolerance
                    time_limit=time_limit,
                )
            else:
                print(
                    f"Warning: Strategies {strategy1} and/or {strategy2} not found in results for {solver_combo}"
                )

        print(f"Analysis for {solver_combo} complete!")

    print("\nAll analysis complete!")


if __name__ == "__main__":
    main() 